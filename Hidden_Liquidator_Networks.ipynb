{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd805b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.colors as mcolors\n",
    "import itertools\n",
    "import matplotlib.ticker as mtick  # Import the formatter for percentages\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "from matplotlib.ticker import FixedLocator\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "# Set the font family to serif\n",
    "plt.rcParams['font.family'] = 'Times New Roman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a676af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.colors as mcolors\n",
    "import itertools\n",
    "import matplotlib.ticker as mtick  # Import the formatter for percentages\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "from matplotlib.ticker import FixedLocator\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "# Set the font family to 'Times New Roman' for better readability\n",
    "plt.rcParams['font.family'] = 'Times New Roman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c60a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file path for the dataset\n",
    "file_path = r'C:\\Users\\labo\\OneDrive - Nexus365\\dissertation\\LB_events_all_cleaned.csv'\n",
    "\n",
    "# Load the dataset with appropriate data types and date parsing\n",
    "dtype_dict = {\"receive_receipt_token\": \"object\"}\n",
    "parse_dates = [\"datetime\"]\n",
    "lb = pd.read_csv(file_path, dtype=dtype_dict, parse_dates=parse_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ec896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the dataset to match the same period as used for correlation analysis\n",
    "lb = lb[lb['datetime'] <= '2023-08-19 23:59:59']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301726c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing only 'liquidation' records\n",
    "liquidations_df = lb[lb['type'] == 'liquidation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns containing NaN values from the 'liquidations_df'\n",
    "liquidations_df = liquidations_df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fe5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all unique addresses from the 'liquidation_caller_address' column\n",
    "unique_liquidators = liquidations_df['liquidation_caller_address'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f7f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from addresses to integer values starting from 1\n",
    "# (early liquidators have lower numbers)\n",
    "address_mapping = {address: i + 1 for i, address in enumerate(unique_liquidators)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbfe7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute the addresses in the 'liquidation_caller_address' column with their mapped values\n",
    "liquidations_df['liquidation_caller_address'] = liquidations_df['liquidation_caller_address'].map(address_mapping)\n",
    "\n",
    "# Create the new column 'block_number_ordering', which is derived from 'block_number' and 'log_index' divided by 10000\n",
    "liquidations_df['block_number_ordering'] = liquidations_df['block_number'] + liquidations_df['log_index'] / 10000\n",
    "\n",
    "# Sort the DataFrame by 'asset_symbol' and 'block_number_ordering'\n",
    "liquidations_df_sorted = liquidations_df.sort_values(by=['asset_symbol', 'block_number_ordering'])\n",
    "\n",
    "# Compute the difference between consecutive 'block_number_ordering' values within each 'asset_symbol' group\n",
    "liquidations_df_sorted['block_diff'] = liquidations_df_sorted.groupby('asset_symbol')['block_number_ordering'].diff()\n",
    "\n",
    "# Identify rows where the difference ('block_diff') is less than or equal to 25 blocks (equivalent to 5 minutes)\n",
    "liquidations_df_sorted['part_of_wave_tail'] = liquidations_df_sorted['block_diff'] <= 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c469a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 'part_of_wave_head' column, which is a shifted version of 'part_of_wave_tail'\n",
    "liquidations_df_sorted['part_of_wave_head'] = liquidations_df_sorted['part_of_wave_tail'].shift(-1)\n",
    "\n",
    "# Identify rows that are part of a wave (either part of the wave tail or wave head)\n",
    "liquidations_df_sorted['part_of_wave'] = liquidations_df_sorted['part_of_wave_tail'] | liquidations_df_sorted['part_of_wave_head']\n",
    "\n",
    "# Drop the 'part_of_wave_head' column\n",
    "liquidations_df_sorted.drop(columns=['part_of_wave_head'], inplace=True)\n",
    "\n",
    "# Add columns to help identify the start of a wave\n",
    "liquidations_df_sorted['next_part_of_wave_tail'] = liquidations_df_sorted['part_of_wave_tail'].shift(-1)\n",
    "liquidations_df_sorted['next_asset_symbol'] = liquidations_df_sorted['asset_symbol'].shift(-1)\n",
    "\n",
    "# Identify the start of a wave based on the given conditions\n",
    "liquidations_df_sorted['wave_start'] = (~liquidations_df_sorted['part_of_wave_tail']) & (liquidations_df_sorted['next_part_of_wave_tail']) & (liquidations_df_sorted['asset_symbol'] == liquidations_df_sorted['next_asset_symbol'])\n",
    "\n",
    "# Drop the temporary columns used for the calculation\n",
    "liquidations_df_sorted.drop(columns=['next_part_of_wave_tail', 'next_asset_symbol'], inplace=True)\n",
    "\n",
    "# Create a 'wave_ID' column by cumulatively summing the 'wave_start' column\n",
    "liquidations_df_sorted['wave_ID'] = liquidations_df_sorted['wave_start'].cumsum()\n",
    "\n",
    "# Set 'wave_ID' to NaN for rows that are not part of a wave\n",
    "liquidations_df_sorted.loc[~liquidations_df_sorted['part_of_wave'], 'wave_ID'] = np.nan\n",
    "\n",
    "total_liquidations = liquidations_df_sorted.shape[0]\n",
    "total_wave_liquidations = liquidations_df_sorted['part_of_wave'].sum()\n",
    "total_wave_liquidations_relative = total_wave_liquidations / total_liquidations\n",
    "total_waves = liquidations_df_sorted['wave_ID'].nunique()\n",
    "\n",
    "# Find the maximum number of liquidations in a wave\n",
    "# Group by 'wave_ID' and count the number of liquidations in each wave\n",
    "liquidations_per_wave = liquidations_df_sorted.groupby('wave_ID').size()\n",
    "max_liquidations_in_wave = liquidations_per_wave.max()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total number of liquidations: {total_liquidations}\")\n",
    "print(f\"Total number of waves: {total_waves}\")\n",
    "print(f\"Total number of liquidations that are part of a wave: {total_wave_liquidations} (or {total_wave_liquidations_relative:.2%} of total liquidations)\")\n",
    "print(f\"Number of liquidations in the wave with the most liquidations: {max_liquidations_in_wave}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93655b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows that are part of a wave\n",
    "wave_liquidations = liquidations_df_sorted[liquidations_df_sorted['part_of_wave']]\n",
    "\n",
    "# Extract the relevant columns ('datetime', 'block_number', 'asset_symbol', 'wave_ID', 'liquidation_caller_address') and reset the index\n",
    "wave_liquidations_long = wave_liquidations[['datetime', 'block_number', 'asset_symbol', 'wave_ID', 'liquidation_caller_address']].reset_index()\n",
    "\n",
    "# Add a new column 'position_in_wave', which represents the position of the address in that wave\n",
    "wave_liquidations_long['position_in_wave'] = wave_liquidations_long.groupby('wave_ID').cumcount() + 1\n",
    "\n",
    "# Rearrange the columns for better readability\n",
    "wave_liquidations_long = wave_liquidations_long[['wave_ID', 'liquidation_caller_address', 'position_in_wave', 'datetime', 'block_number', 'asset_symbol']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b83115",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Approach: Identifying connections\n",
    "\n",
    "# Group by 'liquidation_caller_address' and count the number of liquidations for each liquidator\n",
    "liquidator_stats = liquidations_df.groupby('liquidation_caller_address').size().reset_index(name='total_liquidations')\n",
    "\n",
    "# Count the number of times each address was part of a wave\n",
    "wave_counts = wave_liquidations_long['liquidation_caller_address'].value_counts().reset_index()\n",
    "wave_counts.columns = ['liquidation_caller_address', 'total_wave_liquidations']\n",
    "\n",
    "# Merge this count with the original DataFrame 'liquidator_stats'\n",
    "liquidator_stats = pd.merge(liquidator_stats, wave_counts, on='liquidation_caller_address', how='left')\n",
    "\n",
    "# Fill NaN values with 0 for addresses that were never part of a wave\n",
    "liquidator_stats['total_wave_liquidations'].fillna(0, inplace=True)\n",
    "liquidator_stats['total_wave_liquidations'] = liquidator_stats['total_wave_liquidations'].astype(int)\n",
    "\n",
    "# Re-filter addresses for the most active 20% liquidators based on specific criteria\n",
    "selected_addresses = liquidator_stats[(liquidator_stats['total_liquidations'] >= 22) & (liquidator_stats['total_wave_liquidations'] >= 9)]['liquidation_caller_address'].tolist()\n",
    "filtered_waves = wave_liquidations_long[wave_liquidations_long['liquidation_caller_address'].isin(selected_addresses)] # Filter only the rows of the selected callers\n",
    "\n",
    "# Constructing the graph with 2^-distance weight decay within 5 blocks distance\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for address in selected_addresses:\n",
    "    address_waves = filtered_waves[filtered_waves['liquidation_caller_address'] == address]\n",
    "    \n",
    "    for _, row in address_waves.iterrows():\n",
    "        wave_id = row['wave_ID']\n",
    "        block_num = row['block_number']\n",
    "        \n",
    "        subsequent_rows = filtered_waves[(filtered_waves['wave_ID'] == wave_id) & \n",
    "                                         (filtered_waves['block_number'] > block_num) & \n",
    "                                         (filtered_waves['block_number'] <= block_num + 5)]\n",
    "        \n",
    "        for _, subsequent_row in subsequent_rows.iterrows():\n",
    "            subsequent_address = subsequent_row['liquidation_caller_address']\n",
    "            distance = subsequent_row['block_number'] - block_num\n",
    "            weight = 2**(-distance)\n",
    "            \n",
    "            if G.has_edge(address, subsequent_address):\n",
    "                G[address][subsequent_address]['weight'] += weight\n",
    "            else:\n",
    "                G.add_edge(address, subsequent_address, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate in-degrees and out-degrees for each node\n",
    "in_degrees = dict(G.in_degree(weight='weight'))\n",
    "out_degrees = dict(G.out_degree(weight='weight'))\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "degree_df = pd.DataFrame({\n",
    "    'In_Degree': in_degrees,\n",
    "    'Out_Degree': out_degrees,\n",
    "    'Net_Degree': {node: out_degrees[node] - in_degrees[node] for node in G.nodes()},\n",
    "    'Abs_Degree': {node: out_degrees[node] + in_degrees[node] for node in G.nodes()}\n",
    "}).sort_values(by='Abs_Degree', ascending=False)\n",
    "\n",
    "# Generate the adjacency matrix for the graph\n",
    "adjacency_matrix = nx.adjacency_matrix(G)\n",
    "\n",
    "# Convert the adjacency matrix to a DataFrame for better readability\n",
    "adjacency_df = pd.DataFrame(adjacency_matrix.todense(), index=G.nodes(), columns=G.nodes())\n",
    "\n",
    "# Create adjacency_normalised_out_df by dividing rows by 'Out_Degree' values\n",
    "adjacency_normalised_out_df = adjacency_df.divide(degree_df['Out_Degree'].reindex(adjacency_df.index).values, axis=0)\n",
    "\n",
    "adjacency_normalised_out_df.fillna(0, inplace=True)\n",
    "\n",
    "# Convert columns of adjacency_df to integers for matching with degree_df\n",
    "adjacency_df.columns = adjacency_df.columns.astype(int)\n",
    "\n",
    "# Perform vectorized normalization\n",
    "adjacency_normalised_in_df = adjacency_df.divide(degree_df['In_Degree'].reindex(adjacency_df.columns).values, axis=1)\n",
    "\n",
    "adjacency_normalised_in_df.fillna(0, inplace=True)\n",
    "\n",
    "# Identify indices with 'Abs_Degree' greater than the median\n",
    "median_abs_degree = degree_df['Abs_Degree'].median()\n",
    "high_abs_degree_indices = degree_df[degree_df['Abs_Degree'] > median_abs_degree].index\n",
    "\n",
    "# Filter adjacency_normalised_in_df and adjacency_normalised_out_df based on these indices\n",
    "filtered_adjacency_normalised_in_df = adjacency_normalised_in_df.loc[high_abs_degree_indices, high_abs_degree_indices]\n",
    "filtered_adjacency_normalised_out_df = adjacency_normalised_out_df.loc[high_abs_degree_indices, high_abs_degree_indices]\n",
    "\n",
    "# Replace diagonal entries with 0s for both dataframes\n",
    "filtered_adjacency_normalised_in_df.values[np.diag_indices_from(filtered_adjacency_normalised_in_df)] = 0\n",
    "filtered_adjacency_normalised_out_df.values[np.diag_indices_from(filtered_adjacency_normalised_out_df)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4db3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the graph G_in\n",
    "\n",
    "# Create a directed graph based on filtered_adjacency_normalised_in_df\n",
    "G_in = nx.DiGraph()\n",
    "\n",
    "# Add edges where cell value is at least 1/3\n",
    "for i, row in filtered_adjacency_normalised_in_df.iterrows():\n",
    "    for j, value in row.iteritems():\n",
    "        if value >= 0.3:\n",
    "            G_in.add_edge(i, j, weight=value)\n",
    "\n",
    "# Set the font family to 'serif' for better readability\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Create a color map based on 'total_wave_liquidations' values\n",
    "color_map_in = []\n",
    "for node in G_in.nodes:\n",
    "    liquidations = liquidator_stats.loc[liquidator_stats['liquidation_caller_address'] == node, 'total_wave_liquidations']\n",
    "    if len(liquidations) > 0:\n",
    "        liquidations = liquidations.iloc[0]\n",
    "        if liquidations < 250:\n",
    "            color_map_in.append('lightgrey')\n",
    "        elif liquidations < 500:\n",
    "            color_map_in.append('skyblue')\n",
    "        else:\n",
    "            color_map_in.append((0.27, 0.51, 0.71, 0.9))  # Steelblue with alpha 90%\n",
    "    else:\n",
    "        color_map_in.append('gray')  # Default color for nodes not in liquidator_stats\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 10))  # Adjust the figure size for better visualization\n",
    "pos_in = nx.circular_layout(G_in)\n",
    "\n",
    "# Separate edges into two groups based on weight\n",
    "dashed_edges_in = [(u, v) for u, v, value in G_in.edges.data('weight') if value < 0.5]\n",
    "solid_edges_in = [(u, v) for u, v, value in G_in.edges.data('weight') if value >= 0.5]\n",
    "\n",
    "# Draw solid edges\n",
    "nx.draw_networkx_edges(G_in, pos_in, edgelist=solid_edges_in, width=2.0, alpha=0.5, edge_color='black', style='solid', arrowstyle='-|>', arrowsize=10, min_target_margin=11, min_source_margin=12)\n",
    "\n",
    "# Draw dashed edges\n",
    "nx.draw_networkx_edges(G_in, pos_in, edgelist=dashed_edges_in, width=2.0, alpha=0.5, edge_color='black', style='dashed', arrowstyle='-|>', arrowsize=10, min_target_margin=11, min_source_margin=12)\n",
    "\n",
    "# Draw nodes with color based on color_map\n",
    "nx.draw_networkx_nodes(G_in, pos_in, node_size=500, node_color=color_map_in)\n",
    "nx.draw_networkx_labels(G_in, pos_in, font_size=10, font_family='serif')\n",
    "\n",
    "# Create custom legend items\n",
    "legend_items_in = [Line2D([0], [0], color='black', alpha=0.5, linestyle='dashed', label='$\\mathregular{Weight \\in [0.3,0.5]}$'),\n",
    "                Line2D([0], [0], color='black', alpha=0.5, linestyle='-', label='$\\mathregular{Weight \\in (0.5,1]}$')]\n",
    "\n",
    "# Add color-coded nodes information to the legend\n",
    "legend_items_in += [Line2D([0], [0], marker='o', color='lightgrey', label='Wave liquidations < 250', markersize=10, linestyle='None'),\n",
    "                 Line2D([0], [0], marker='o', color='skyblue', label='$\\mathregular{Wave\\ liquidations \\in [250,500)}$', markersize=10, linestyle='None'),\n",
    "                 Line2D([0], [0], marker='o', color='steelblue', label='$\\mathregular{Wave\\ liquidations \\geq 500}$', markersize=10, linestyle='None')]\n",
    "\n",
    "plt.legend(handles=legend_items_in, loc='center', fontsize='x-large')  # Add the legend\n",
    "\n",
    "# Turn off the axis frame\n",
    "plt.axis('off')\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "# plt.savefig(r'C:\\Users\\labo\\OneDrive - Nexus365\\Diss\\exploratory analsis\\plots\\G_in.png', bbox_inches='tight', pad_inches=0.01)\n",
    "\n",
    "# plt.title(\"Directed Graph from filtered_adjacency_normalised_in_df\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff284259",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the graph G_out\n",
    "\n",
    "# Create a directed graph based on filtered_adjacency_normalised_out_df\n",
    "G_out = nx.DiGraph()\n",
    "\n",
    "# Add edges where cell value is at least 1/3\n",
    "for i, row in filtered_adjacency_normalised_out_df.iterrows():\n",
    "    for j, value in row.iteritems():\n",
    "        if value > 0.3:\n",
    "            G_out.add_edge(i, j, weight=value)\n",
    "\n",
    "# Set the font family to 'serif' for better readability\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Create a color map based on 'total_wave_liquidations' values\n",
    "color_map_out = []\n",
    "for node in G_out.nodes:\n",
    "    liquidations = liquidator_stats.loc[liquidator_stats['liquidation_caller_address'] == node, 'total_wave_liquidations']\n",
    "    if len(liquidations) > 0:\n",
    "        liquidations = liquidations.iloc[0]\n",
    "        if liquidations < 250:\n",
    "            color_map_out.append('lightgrey')\n",
    "        elif liquidations < 500:\n",
    "            color_map_out.append('skyblue')\n",
    "        else:\n",
    "            color_map_out.append((0.27, 0.51, 0.71, 0.9))  # Steelblue with alpha 90%\n",
    "    else:\n",
    "        color_map_out.append('gray')  # Default color for nodes not in liquidator_stats\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 10))  # Adjust the figure size for better visualization\n",
    "pos_out = nx.circular_layout(G_out)\n",
    "\n",
    "# Separate edges into two groups based on weight\n",
    "dashed_edges_out = [(u, v) for u, v, value in G_out.edges.data('weight') if value < 0.5]\n",
    "solid_edges_out = [(u, v) for u, v, value in G_out.edges.data('weight') if value >= 0.5]\n",
    "\n",
    "# Draw solid edges\n",
    "nx.draw_networkx_edges(G_out, pos_out, edgelist=solid_edges_out, width=2.0, alpha=0.5, edge_color='black', style='solid', arrowstyle='-|>', arrowsize=10, min_target_margin=11, min_source_margin=12)\n",
    "\n",
    "# Draw dashed edges\n",
    "nx.draw_networkx_edges(G_out, pos_out, edgelist=dashed_edges_out, width=2.0, alpha=0.5, edge_color='black', style='dashed', arrowstyle='-|>', arrowsize=10, min_target_margin=11, min_source_margin=12)\n",
    "\n",
    "# Draw nodes with color based on color_map\n",
    "nx.draw_networkx_nodes(G_out, pos_out, node_size=500, node_color=color_map_out)\n",
    "nx.draw_networkx_labels(G_out, pos_out, font_size=10, font_family='serif')\n",
    "\n",
    "# Create custom legend items\n",
    "legend_items_out = [Line2D([0], [0], color='black', linestyle='dashed', label='$\\mathregular{Weight \\in [0.3,0.5]}$'),\n",
    "                Line2D([0], [0], color='black', linestyle='-', label='$\\mathregular{Weight \\in (0.5,1]}$')]\n",
    "\n",
    "# Add color-coded nodes information to the legend\n",
    "legend_items_out += [Line2D([0], [0], marker='o', color='lightgrey', label='Wave liquidations < 250', markersize=10, linestyle='None'),\n",
    "                 Line2D([0], [0], marker='o', color='skyblue', label='$\\mathregular{Wave\\ liquidations \\in [250,500)}$', markersize=10, linestyle='None'),\n",
    "                 Line2D([0], [0], marker='o', color='steelblue', label='$\\mathregular{Wave\\ liquidations \\geq 500}$', markersize=10, linestyle='None')]\n",
    "\n",
    "plt.legend(handles=legend_items_out, loc='center', fontsize='x-large')  # Add the legend\n",
    "\n",
    "# Turn off the axis frame\n",
    "plt.axis('off')\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "# plt.savefig(r'C:\\Users\\labo\\OneDrive - Nexus365\\Diss\\exploratory analsis\\plots\\G_out.png', bbox_inches='tight', pad_inches=0.01)\n",
    "\n",
    "# plt.title(\"Directed Graph from filtered_adjacency_normalised_out_df\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c532de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the combined graph G_inout as an undirected graph\n",
    "G_inout = nx.Graph()\n",
    "\n",
    "# Create a set to hold the undirected edges that should be added to G_inout\n",
    "edges_inout = set()\n",
    "\n",
    "# Iterate through the directed edges in G_in\n",
    "for u, v in G_in.edges():\n",
    "    # Check if the opposite-directed edge exists in G_out\n",
    "    if G_out.has_edge(v, u):\n",
    "        edges_inout.add((u, v))\n",
    "\n",
    "# Iterate through the directed edges in G_out\n",
    "for u, v in G_out.edges():\n",
    "    # Check if the opposite-directed edge exists in G_in\n",
    "    if G_in.has_edge(v, u):\n",
    "        edges_inout.add((u, v))\n",
    "\n",
    "# Add the edges to G_inout\n",
    "G_inout.add_edges_from(edges_inout)\n",
    "\n",
    "# Set the font family to 'serif' for better readability\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "\n",
    "# Create the color map based on the total_wave_liquidations values\n",
    "color_map_inout = []\n",
    "for node in G_inout.nodes():\n",
    "    liquidations = liquidator_stats.loc[liquidator_stats['liquidation_caller_address'] == node, 'total_wave_liquidations']\n",
    "    if len(liquidations) > 0:\n",
    "        liquidations = liquidations.iloc[0]\n",
    "        if liquidations < 250:\n",
    "            color_map_inout.append('lightgrey')\n",
    "        elif liquidations < 500:\n",
    "            color_map_inout.append('skyblue')\n",
    "        else:\n",
    "            color_map_inout.append('steelblue')\n",
    "    else:\n",
    "        color_map_inout.append('gray')  # Default color for nodes not in liquidator_stats\n",
    "\n",
    "# Plotting the graph\n",
    "plt.figure(figsize=(27, 9))  # Adjust the figure size for better visualization\n",
    "pos_inout = nx.spring_layout(G_inout, k=0.65, seed=168)  # Layout for the nodes\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(G_inout, pos_inout, node_color=color_map_inout, edge_color=((0, 0, 0, 0.5)), width=4, with_labels=True, node_size=5000, font_size=30, font_family='serif')\n",
    "\n",
    "# Add the legend\n",
    "legend_items_inout = [Line2D([0], [0], marker='o', color='lightgrey', label='Wave liquidations < 250', markersize=30, linestyle='None'),\n",
    "                     Line2D([0], [0], marker='o', color='skyblue', label='$\\mathregular{Wave\\ liquidations \\in [250,500)}$', markersize=30, linestyle='None'),\n",
    "                     Line2D([0], [0], marker='o', color='steelblue', label='$\\mathregular{Wave\\ liquidations \\geq 500}$', markersize=30, linestyle='None')]\n",
    "\n",
    "plt.legend(handles=legend_items_inout, loc='best', frameon=True, facecolor='none')\n",
    "#plt.title(\"Combined Undirected Graph from G_in and G_out\")\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "# plt.savefig(r'C:\\Users\\labo\\OneDrive - Nexus365\\Diss\\exploratory analsis\\plots\\G_hat.png', bbox_inches='tight', pad_inches=0.01)\n",
    "\n",
    "# plt.title(\"Directed Graph from filtered_adjacency_normalised_out_df\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceacbf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Approach: Identifying initiators\n",
    "# Filter records where position_in_wave is between 1 and 5\n",
    "filtered_wave_liquidations = wave_liquidations_long[wave_liquidations_long['position_in_wave'].isin(range(1, 6))]\n",
    "\n",
    "# Pivot the table to count positions for each address\n",
    "position_counts = filtered_wave_liquidations.pivot_table(index='liquidation_caller_address', columns='position_in_wave', values='wave_ID', aggfunc='count').reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "position_counts.columns = ['liquidation_caller_address', 'position_1', 'position_2', 'position_3', 'position_4', 'position_5']\n",
    "\n",
    "# Merge position counts with total liquidations\n",
    "liquidator_stats = liquidator_stats.merge(position_counts, on='liquidation_caller_address', how='left').fillna(0)\n",
    "\n",
    "# Order the dataframe by position 1 counts in descending order\n",
    "liquidator_stats = liquidator_stats.sort_values(by='position_1', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Calculate relative positions\n",
    "liquidator_stats['position_1_relative'] = liquidator_stats['position_1'] / liquidator_stats['total_wave_liquidations']\n",
    "liquidator_stats['position_1to3_relative'] = (liquidator_stats['position_1'] + liquidator_stats['position_2'] + liquidator_stats['position_3']) / liquidator_stats['total_wave_liquidations']\n",
    "liquidator_stats['position_1to5_relative'] = (liquidator_stats['position_1'] + liquidator_stats['position_2'] + liquidator_stats['position_3'] + liquidator_stats['position_4'] + liquidator_stats['position_5']) / liquidator_stats['total_wave_liquidations']\n",
    "\n",
    "# Compute additional columns for \"liquidator_stats\"\n",
    "\n",
    "# Calculate 'total_isolated_liquidations'\n",
    "liquidator_stats['total_isolated_liquidations'] = liquidator_stats['total_liquidations'] - liquidator_stats['total_wave_liquidations']\n",
    "\n",
    "# Calculate 'min_position' and 'max_position'\n",
    "liquidator_stats_min_positions = wave_liquidations_long.groupby('liquidation_caller_address')['position_in_wave'].min()\n",
    "liquidator_stats_max_positions = wave_liquidations_long.groupby('liquidation_caller_address')['position_in_wave'].max()\n",
    "liquidator_stats = liquidator_stats.merge(liquidator_stats_min_positions.rename('min_position'), left_on='liquidation_caller_address', right_index=True, how='left')\n",
    "liquidator_stats = liquidator_stats.merge(liquidator_stats_max_positions.rename('max_position'), left_on='liquidation_caller_address', right_index=True, how='left')\n",
    "\n",
    "# Calculate 'total_waves'\n",
    "liquidator_stats_total_waves = wave_liquidations_long.groupby('liquidation_caller_address')['wave_ID'].nunique()\n",
    "liquidator_stats = liquidator_stats.merge(liquidator_stats_total_waves.rename('total_waves'), left_on='liquidation_caller_address', right_index=True, how='left')\n",
    "\n",
    "# Calculate 'own_follower'\n",
    "wave_liquidations_long['next_address'] = wave_liquidations_long.groupby('wave_ID')['liquidation_caller_address'].shift(-1)\n",
    "liquidator_stats_own_followers = wave_liquidations_long[wave_liquidations_long['liquidation_caller_address'] == wave_liquidations_long['next_address']].groupby('liquidation_caller_address').size()\n",
    "liquidator_stats = liquidator_stats.merge(liquidator_stats_own_followers.rename('own_follower'), left_on='liquidation_caller_address', right_index=True, how='left')\n",
    "liquidator_stats['own_follower'].fillna(0, inplace=True)\n",
    "\n",
    "# Define a function to find the longest run of consecutive addresses\n",
    "def longest_run_of_address(group):\n",
    "    addresses = group['liquidation_caller_address'].tolist()\n",
    "    \n",
    "    # Identify the longest run of consecutive addresses\n",
    "    liquidator_stats_longest_run = max((sum(1 for _ in g) for k, g in itertools.groupby(addresses)), default=0)\n",
    "    return liquidator_stats_longest_run\n",
    "\n",
    "# Compute the longest run for each address across all waves\n",
    "liquidator_stats_longest_runs = wave_liquidations_long.groupby(['wave_ID', 'liquidation_caller_address']).apply(longest_run_of_address).groupby('liquidation_caller_address').max()\n",
    "liquidator_stats = liquidator_stats.merge(liquidator_stats_longest_runs.rename('longest_run'), left_on='liquidation_caller_address', right_index=True, how='left')\n",
    "\n",
    "# Calculate additional columns for \"liquidator_stats\"\n",
    "\n",
    "# Calculate 'avg_liquidations_per_wave'\n",
    "liquidator_stats['avg_liquidations_per_wave'] = liquidator_stats['total_wave_liquidations'] / liquidator_stats['total_waves']\n",
    "\n",
    "# Define a function to compute 'total_runs'\n",
    "def total_runs_of_address(group):\n",
    "    addresses = group['liquidation_caller_address'].tolist()\n",
    "    \n",
    "    # Identify the runs of consecutive addresses\n",
    "    runs = [(k, sum(1 for _ in g)) for k, g in itertools.groupby(addresses)]\n",
    "    \n",
    "    # Filter for runs of length 2 or more and count them\n",
    "    return [k for k, length in runs if length >= 2]\n",
    "\n",
    "# Compute the total runs for each address across all waves\n",
    "liquidator_stats_total_runs = wave_liquidations_long.groupby('wave_ID').apply(total_runs_of_address)\n",
    "liquidator_stats_total_runs_list = [item for sublist in liquidator_stats_total_runs for item in sublist]\n",
    "\n",
    "# Count occurrences for each address\n",
    "total_runs = pd.Series(liquidator_stats_total_runs_list).value_counts()\n",
    "\n",
    "# Merge with liquidator_stats\n",
    "liquidator_stats = liquidator_stats.merge(total_runs.rename('total_runs'), left_on='liquidation_caller_address', right_index=True, how='left')\n",
    "liquidator_stats['total_runs'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculate 'total_run_liquidations'\n",
    "liquidator_stats['total_run_liquidations'] = liquidator_stats['total_runs'] + liquidator_stats['own_follower']\n",
    "\n",
    "# Define a function to get top 3 addresses and the rest for a given position column\n",
    "def get_top_5_and_rest(df, column_name):\n",
    "    \"\"\"\n",
    "    Extract top 5 addresses based on a column and group the rest.\n",
    "    \"\"\"\n",
    "    top_5 = df.nlargest(5, column_name)\n",
    "    rest_value = df[column_name].sum() - top_5[column_name].sum()\n",
    "    values = top_5[column_name].tolist() + [rest_value]\n",
    "    labels = top_5['liquidation_caller_address'].astype(str).tolist() + ['Others']\n",
    "    percentages = [f\"{(val/df[column_name].sum())*100:.1f}%\" for val in values]\n",
    "    return values, labels, percentages\n",
    "\n",
    "# Define a plotting function\n",
    "def plot_pie_chart(ax, values, labels, percentages, title):\n",
    "    ax.pie(values, labels=[f\"{label} ({percentage})\" for label, percentage in zip(labels, percentages)], \n",
    "           startangle=90, colors=['steelblue', 'powderblue', 'skyblue', 'gray', 'darkgray', 'lightgray'],\n",
    "           labeldistance=1.1, pctdistance=0.8)\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Extract data for each position\n",
    "values_1, labels_1, percentages_1 = get_top_5_and_rest(liquidator_stats, 'position_1')\n",
    "values_2, labels_2, percentages_2 = get_top_5_and_rest(liquidator_stats, 'position_2')\n",
    "values_3, labels_3, percentages_3 = get_top_5_and_rest(liquidator_stats, 'position_3')\n",
    "values_4, labels_4, percentages_4 = get_top_5_and_rest(liquidator_stats, 'position_4')\n",
    "values_5, labels_5, percentages_5 = get_top_5_and_rest(liquidator_stats, 'position_5')\n",
    "\n",
    "# Plot the pie charts\n",
    "fig, axs = plt.subplots(1, 2, figsize=(27, 10))\n",
    "\n",
    "# Adjust the horizontal space between subplots\n",
    "plt.subplots_adjust(wspace=2)\n",
    "\n",
    "plot_pie_chart(axs[0], values_1, labels_1, percentages_1, 'Position 1')\n",
    "plot_pie_chart(axs[1], values_2, labels_2, percentages_2, 'Position 2')\n",
    "#plot_pie_chart(axs[2], values_3, labels_3, percentages_3, 'Position 3')\n",
    "#plot_pie_chart(axs[3], values_4, labels_4, percentages_4, 'Position 4')\n",
    "#plot_pie_chart(axs[4], values_5, labels_5, percentages_5, 'Position 5')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd23a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different starting tuples are there?\n",
    "\n",
    "# Extract the first two nodes for each wave\n",
    "first_two_nodes = wave_liquidations_long[wave_liquidations_long['position_in_wave'].isin([1, 2])]\n",
    "\n",
    "# Group by wave_ID and aggregate the liquidation_caller_address into a tuple\n",
    "starting_tuples = first_two_nodes.groupby('wave_ID')['liquidation_caller_address'].apply(tuple)\n",
    "\n",
    "# Total number of UNIQUE starting tuples\n",
    "num_unique_starting_tuples = len(set(starting_tuples))\n",
    "print(f\"Number of different starting tuples: {num_unique_starting_tuples}\")\n",
    "\n",
    "# How many double tuples (of the form (X,X)) exist?\n",
    "\n",
    "# Count tuples with the same number for both positions\n",
    "double_tuple_count = sum(1 for t in set(starting_tuples) if t[0] == t[1])\n",
    "print(f\"Number of double tuples: {double_tuple_count}\")\n",
    "\n",
    "# How many double tuples start a liquidation wave?\n",
    "\n",
    "# Count the total occurrences of the repeated tuples in the original tuples series\n",
    "double_tuple_occurrences = starting_tuples[starting_tuples.apply(lambda x: x[0] == x[1])].count()\n",
    "print(f\"Double tuples occur at {double_tuple_occurrences} or {(double_tuple_occurrences / total_waves):.2%} of liquidation wave starts.\")\n",
    "\n",
    "# Filter the tuples to get only the repeated tuples, get the counts and the top 10 addresses\n",
    "top_10_starting_tuples = starting_tuples[starting_tuples.apply(lambda x: x[0] == x[1])].value_counts().head(10)\n",
    "\n",
    "# Calculate the cumulative share\n",
    "cumulative_share = top_10_starting_tuples.cumsum() / double_tuple_occurrences\n",
    "\n",
    "# Convert the index elements to strings using a list comprehension\n",
    "x_labels = [str(item) for item in top_10_starting_tuples.index]\n",
    "\n",
    "# Create a figure with two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(21, 14))\n",
    "ax1.set_axisbelow(True)\n",
    "\n",
    "# Create the bar chart on the first y-axis\n",
    "ax1.bar(x_labels, top_10_starting_tuples.values, color='skyblue')\n",
    "ax1.set_xlabel('Liquidator', labelpad=10)\n",
    "ax1.set_ylabel('Number of Occurrences', labelpad=10)\n",
    "ax1.tick_params(axis='y')\n",
    "ax1.set_xticklabels([label.split(',')[0].strip('()') for label in x_labels], rotation=45)\n",
    "ax1.tick_params(axis='x', which='major', length=20, width=4)\n",
    "\n",
    "# Add a grid on the first y-axis\n",
    "ax1.grid(axis='y', linestyle=':', linewidth=3)\n",
    "ax1.tick_params(axis='y', which='major', length=20, width=4)\n",
    "\n",
    "# Create the line plot on the second y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x_labels, cumulative_share, marker='o', markersize=16, linewidth=5, color='grey', linestyle='-', label='Cumulative Share')\n",
    "ax2.set_ylabel('Cumulative Share')\n",
    "ax2.tick_params(axis='y')\n",
    "ax2.set_ylim(0, 1)  # Ensure the y-axis for cumulative share ranges from 0 to 1\n",
    "ax2.tick_params(axis='y', which='major', length=20, width=4)\n",
    "\n",
    "# Format the y-tick labels of the second axis as percentages\n",
    "ax2.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0, decimals=0))\n",
    "\n",
    "# Add a legend\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directed graph G_1: Universal weight of +1 for the first edge only after wave start (excluding self-edges)\n",
    "G_1 = nx.DiGraph()\n",
    "\n",
    "# Iterate over the wave_liquidations_long dataframe grouped by wave_ID\n",
    "for _, group in wave_liquidations_long.sort_values(by='position_in_wave').groupby('wave_ID'):\n",
    "    # Get the first two nodes of each wave\n",
    "    nodes = group['liquidation_caller_address'].tolist()[:2]\n",
    "    source, target = nodes\n",
    "\n",
    "    # Avoid self-loops\n",
    "    if source != target:\n",
    "        # Check if the edge already exists\n",
    "        if G_1.has_edge(source, target):\n",
    "            # Increase the weight by +1\n",
    "            G_1[source][target]['weight'] += 1\n",
    "        else:\n",
    "            # Add a new edge with weight = 1\n",
    "            G_1.add_edge(source, target, weight=1)\n",
    "\n",
    "# Number of nodes and edges in the updated G_1\n",
    "num_nodes_G1 = G_1.number_of_nodes()\n",
    "num_edges_G1 = G_1.number_of_edges()\n",
    "print(f\"Number of nodes in G_1: {num_nodes_G1}\")\n",
    "print(f\"Number of edges in G_1: {num_edges_G1}\")\n",
    "\n",
    "# Plotting the directed graph G_1\n",
    "plt.figure(figsize=(15, 15))\n",
    "pos_1 = nx.spring_layout(G_1, k=1.5)\n",
    "# pos_1 = nx.random_layout(G_1)\n",
    "nx.draw_networkx_nodes(G_1, pos_1, node_size=200)\n",
    "nx.draw_networkx_edges(G_1, pos_1, width=1.0, alpha=0.5, arrowstyle='-|>', arrowsize=20, edge_color='grey')\n",
    "nx.draw_networkx_labels(G_1, pos_1, font_size=8)\n",
    "plt.title(\"Directed Graph G_1 from wave_liquidations_long\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda08d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract edges with weight > 1\n",
    "edges_G_1_subgraph = [(u, v) for u, v, d in G_1.edges(data=True) if d[\"weight\"] >= 15]\n",
    "\n",
    "# Create a subgraph from G_1 using the desired edges\n",
    "G_1_subgraph = G_1.edge_subgraph(edges_G_1_subgraph)\n",
    "\n",
    "# Plotting the subgraph with uniform edge widths\n",
    "plt.figure(figsize=(10, 10))\n",
    "pos_G_1_subgraph = nx.circular_layout(G_1_subgraph)\n",
    "nx.draw_networkx(G_1_subgraph, pos=pos_G_1_subgraph, with_labels=True, node_size=500, font_size=10, edge_color=\"black\", width=1, connectionstyle=\"arc3, rad=0.08\")\n",
    "plt.title(\"Subgraph of G_1 with Edges of Weight > 15\")  # Added a title for clarity\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Create color map based on total_wave_liquidations values\n",
    "color_map_G_1_subgraph = []\n",
    "\n",
    "for node in G_1_subgraph.nodes:\n",
    "    liquidations = liquidator_stats.loc[liquidator_stats['liquidation_caller_address'] == node, 'total_wave_liquidations']\n",
    "    if len(liquidations) > 0:\n",
    "        liquidations = liquidations.iloc[0]\n",
    "        if liquidations < 250:\n",
    "            color_map_G_1_subgraph.append('lightgrey')\n",
    "        elif liquidations < 500:\n",
    "            color_map_G_1_subgraph.append('skyblue')\n",
    "        else:\n",
    "            color_map_G_1_subgraph.append((0.27, 0.51, 0.71, 0.9))  # Steelblue with alpha 90%\n",
    "    else:\n",
    "        color_map_G_1_subgraph.append('gray')  # Default color for nodes not in liquidator_stats\n",
    "\n",
    "# Plotting the subgraph with modified edge styles and two separate edge collections\n",
    "plt.figure(figsize=(27, 9))\n",
    "pos_G_1_subgraph = nx.spring_layout(G_1_subgraph, seed=3, k=2.4)\n",
    "\n",
    "# Draw nodes and labels\n",
    "nx.draw_networkx_nodes(G_1_subgraph, pos=pos_G_1_subgraph, node_size=4500, node_color=color_map_G_1_subgraph)\n",
    "nx.draw_networkx_labels(G_1_subgraph, pos=pos_G_1_subgraph, font_size=30, font_family='serif')\n",
    "\n",
    "# Separate edges by weight criteria and draw them with different styles\n",
    "solid_edges_G_1_subgraph = [(u, v) for u, v, d in G_1_subgraph.edges(data=True) if d[\"weight\"] >= 35]\n",
    "dashed_edges_G_1_subgraph = [(u, v) for u, v, d in G_1_subgraph.edges(data=True) if d[\"weight\"] < 35]\n",
    "\n",
    "nx.draw_networkx_edges(G_1_subgraph, pos=pos_G_1_subgraph, edgelist=solid_edges_G_1_subgraph,\n",
    "                       style=\"solid\", connectionstyle=\"arc3,rad=0.1\", width=4, arrowsize=30,\n",
    "                       edge_color='black', alpha=0.5, min_target_margin=30, min_source_margin=32)\n",
    "nx.draw_networkx_edges(G_1_subgraph, pos=pos_G_1_subgraph, edgelist=dashed_edges_G_1_subgraph,\n",
    "                       style=\"dashed\", connectionstyle=\"arc3,rad=0.1\", width=4, arrowsize=30,\n",
    "                       edge_color='black', alpha=0.5, min_target_margin=30, min_source_margin=32)\n",
    "\n",
    "# Adding a legend\n",
    "legend_items_G_1_subgraph = [\n",
    "    Line2D([0], [0], color='black', alpha=0.5, linestyle='dashed', label='$\\mathregular{Weight \\in [15,35)}$'),\n",
    "    Line2D([0], [0], color='black', alpha=0.5, linestyle='-', label='$\\mathregular{Weight \\geq 35}$'),\n",
    "    Line2D([0], [0], marker='o', color='skyblue', label='$\\mathregular{Wave\\ liquidations \\in [250,500)}$', markersize=10, linestyle='None'),\n",
    "    Line2D([0], [0], marker='o', color='steelblue', label='$\\mathregular{Wave\\ liquidations \\geq 500}$', markersize=10, linestyle='None')]\n",
    "\n",
    "plt.legend(handles=legend_items_G_1_subgraph, loc='upper right', frameon=True, facecolor='white', framealpha=0.5).get_frame().set_linewidth(4.0)\n",
    "plt.title(\"Subgraph of G_1 with Edges of Weight > 15\")  # Updated the title\n",
    "plt.axis('off')\n",
    "\n",
    "# Save the plot to the specified directory\n",
    "#plt.savefig(r'C:\\Users\\labo\\OneDrive - Nexus365\\Diss\\exploratory analsis\\plots\\G_1_subgraph.png', bbox_inches='tight', pad_inches=0.01)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
